{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Follow example of Machinelearningmastery.com\n",
    "    \n",
    "Here i follow step by step the implementation of deep learning w/ Keras for sentymen analysis to better understand the tools.\n",
    "\n",
    "Small modifications will be needed in order to implement it to Cajamar problem.\n",
    "\n",
    "Sources: http://machinelearningmastery.com/predict-sentiment-movie-reviews-using-deep-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem desc: The Large Movie Review Dataset (often referred to as the IMDB dataset) contains 25,000 highly polar moving reviews (good or bad) for training and the same amount again for testing. The problem is to determine whether a given moving review has a positive or negative sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description of the data set:** The keras.datasets.imdb.load_data() allows you to load the dataset in a format that is ready for use in neural network and deep learning models.\n",
    "\n",
    "The words have been replaced by integers that indicate the absolute popularity of the word in the dataset. The sentences in each review are therefore comprised of a sequence of integers.\n",
    "\n",
    "That is, every word has associated a unique integer. 1=most popular word, 9999..= least popular word. Or al reves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import preprocessing\n",
    "from keras.datasets import imdb\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load the Data Set\n",
    "(X_train, y_train), (X_test, y_test)= imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "<type 'list'>\n",
      "218\n",
      "189\n",
      "141\n",
      "550\n",
      "147\n"
     ]
    }
   ],
   "source": [
    "print X_train.shape\n",
    "print type(X_train[0])\n",
    "for i in range(5):\n",
    "    print len(X_train[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que es un array multidemiensional con 25K entradas (cada entrada es una review). Cada entrada es una lista de longitud variable, lista de palabras representadas por su correspondiente int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "<type 'numpy.ndarray'>\n",
      "<type 'numpy.int64'>\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print y_train.shape\n",
    "print type(y)\n",
    "print type(y[0])\n",
    "\n",
    "for i in range(5):\n",
    "    print y[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que las y son las labels. Tenemos 25K labels del train, una para cada review. En cada entrada tenemos un numero binario, indica si la review es positiva o negativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "X=np.concatenate((X_train, X_test), axis=0)\n",
    "y=np.concatenate((y_train, y_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000,)\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "#our data\n",
    "print X.shape\n",
    "print y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: \n",
      "[0 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Summarize number of classes.\n",
    "print 'Classes: '\n",
    "print np.unique(y)\n",
    "type(np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that y is a binary array with only two types of entries. 1 or 0. One indicating positive review, the other indicating negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words:  88585\n"
     ]
    }
   ],
   "source": [
    "#Summarize number of words. \n",
    "#Get an idea of how many diferent words are on the data set\n",
    "\n",
    "print 'Number of words: ' , len(np.unique(np.hstack(X))) #hstack stacks (unstacks) horizontally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000,)\n",
      "(11737946,)\n",
      "<type 'list'>\n",
      "<type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "L=np.hstack(X)\n",
    "print X.shape\n",
    "print L.shape\n",
    "print type(X[0])\n",
    "print type(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review lenght: \n",
      "234.75892\n"
     ]
    }
   ],
   "source": [
    "#Get an idea of the average review length\n",
    "print 'Review lenght: '\n",
    "print len(np.hstack(X))/float(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review lenght:\n",
      "Mean:  234.75892  words,  std:  172.911494587\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAg4AAAFkCAYAAABIPLOYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12358abd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Summarize review lenght. HIS CODE:\n",
    "\n",
    "print 'Review lenght:'\n",
    "result= map(len, X) #Returns a list of the results aplying len to the arguments of the sequence\n",
    "print 'Mean: ', np.mean(result), ' words,  std: ', np.std(result)\n",
    "pyplot.boxplot(result)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that the median is 234 words per reviews with a std of 170 words. Looking at the box plot it is clear that we can get most of the reviews whole with a clipped lenght of 400-500 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='purple'> Note: Box and whisker plots. What are they. </font>\n",
    "\n",
    "From docstring: Make a box and whisker plot for each column of ``x`` or each vector in sequence ``x``.  The box extends from the lower to\n",
    "upper quartile values of the data, with a line at the median.\n",
    "The whiskers extend from the box to show the range of the\n",
    "data.  Flier points are those past the end of the whiskers.\n",
    "\n",
    "**From WIKI:**\n",
    "\n",
    "Box and whisker plots are uniform in their use of the box: **the bottom and top of the box are always the first and third quartiles, and the band inside the box is always the second quartile (the median).** But the ends of the whiskers can represent several possible alternative values, among them:\n",
    "\n",
    "- the minimum and maximum of all of the data[1] (as in figure 2)\n",
    "\n",
    "- the lowest datum still within 1.5 IQR of the lower quartile, and the highest datum still within 1.5 IQR of the upper quartile (often called the Tukey boxplot)[2][3] (as in figure 3)\n",
    "\n",
    "- one standard deviation above and below the mean of the data\n",
    "\n",
    "- the 9th percentile and the 91st percentile\n",
    "- the 2nd percentile and the 98th percentile.\n",
    "\n",
    "Any data not included between the whiskers should be plotted as an outlier with a dot, small circle, or star, but occasionally this is not done.\n",
    " <font color='red'> So if i get this correctly 50% of the data is inside the box, the whiskers extend to default = 1.5, 3QR+ 1.5*IQR no idea what this is </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word embeddings**\n",
    "Techniche for natural language processing where words are assigned a positive integer, and a vector in a N dimensional vector space. The embedding layer works its magic and similarty between words is represented as closedness in the vector space, metric is probably the euclidian, (necessarly?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our problem:\n",
    "\n",
    "**Embedding layer.**\n",
    "Maps every word (integer) to a N-dimensional vector space.\n",
    "\n",
    "We will only be interested in the first 5,000 most common words (vocabulary size). The output dimension will be 32, we will use a 32 dimension vector to represent each word. We cap the lenght of the review at 500 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by developing a multi-layer perceptron model with a single hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#MLP for the IMBD problem\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "#fix random seed for ?? reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load dataset with only 5K most common words\n",
    "topwords=5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=topwords) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall fix review lenght at 500, truncating longer ones and and 0-padding shorter ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cut the sequences into maximum fixed lenght of 500\n",
    "max_words=500\n",
    "X_train= sequence.pad_sequences(X_train, maxlen=max_words)#Afegeix 0 al principi o trunca fins a arribar a 500\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create our model. We will use an Embedding layer as the input layer, setting the vocabulary to 5,000, the word vector size to 32 dimensions and the input_length to 500. The output of this first layer will be a 32×500 sized matrix as discussed in the previous section.\n",
    "\n",
    "We will flatten the Embedded layers output to one dimension, then use one dense hidden layer of 250 units with a rectifier activation function. The output layer has one neuron and will use a sigmoid activation to output values of 0 and 1 as predictions.\n",
    "\n",
    "The model uses logarithmic loss and is optimized using the efficient ADAM optimization procedure.\n",
    "\n",
    "<font color = red> Lmao what </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_1 (Embedding)          (None, 500, 32)       160000      embedding_input_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 16000)         0           embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 250)           4000250     flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1)             251         dense_1[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 4,160,501\n",
      "Trainable params: 4,160,501\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Create model.\n",
    "model= Sequential()\n",
    "model.add(Embedding(topwords, 32, input_length=max_words))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the model and use the test as validation while training. This model overfits quickly, (why?) we will only use 2 training epochs. Batch size of 128 since there is a lot of data. We avaluate accuracy on the test dataset.\n",
    "\n",
    "**Epoch:** number of iterations that the training process goes trough the dataset.\n",
    "\n",
    "**Batch size:** Number of instances that are evaluated before a weight update in the network is performed. <font color=red> Still not getting it </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 32s - loss: 0.0734 - acc: 0.9786 - val_loss: 0.3771 - val_acc: 0.8667\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 30s - loss: 0.0142 - acc: 0.9980 - val_loss: 0.4652 - val_acc: 0.8672\n",
      "Accuracy: 0.87%\n"
     ]
    }
   ],
   "source": [
    "#Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=2, batch_size=128, verbose=1)\n",
    "#Final evaluation ?? but we trained with the test. What the hell\n",
    "scores= model.evaluate(X_test, y_test, verbose=0)\n",
    "print 'Accuracy: %.2f% %' % scores[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  87.412 %\n"
     ]
    }
   ],
   "source": [
    "print 'Accuracy: ', scores[1]*100, '%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
